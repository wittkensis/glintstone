<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Resourcing Assessment - Cuneiform Artifact Processing Pipeline</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
            color: #333;
        }
        .container {
            max-width: 1100px;
            margin: 0 auto;
        }
        h1 {
            color: #1a1a2e;
            border-bottom: 3px solid #4a4e69;
            padding-bottom: 10px;
        }
        h2 {
            color: #22223b;
            margin-top: 40px;
            border-left: 4px solid #9a8c98;
            padding-left: 15px;
        }
        h3 {
            color: #4a4e69;
            margin-top: 25px;
        }
        .section {
            background-color: #fff;
            padding: 25px;
            margin-bottom: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        }
        .summary-box {
            background: linear-gradient(135deg, #f5f3f4 0%, #edede9 100%);
            border-left: 4px solid #4a4e69;
            padding: 20px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }
        .warning-box {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px 20px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }
        .info-box {
            background-color: #e7f3ff;
            border-left: 4px solid #0066cc;
            padding: 15px 20px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }
        .success-box {
            background-color: #d4edda;
            border-left: 4px solid #28a745;
            padding: 15px 20px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95em;
        }
        th, td {
            border: 1px solid #dee2e6;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #4a4e69;
            color: white;
            font-weight: 600;
        }
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        tr:hover {
            background-color: #e9ecef;
        }
        code {
            background-color: #e9ecef;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 0.9em;
        }
        .metric {
            font-size: 2em;
            font-weight: bold;
            color: #4a4e69;
        }
        .metric-label {
            font-size: 0.85em;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .metric-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .metric-card {
            background: #fff;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border: 1px solid #dee2e6;
        }
        ul, ol {
            margin-left: 20px;
        }
        li {
            margin-bottom: 8px;
        }
        .phase-header {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .phase-number {
            background-color: #4a4e69;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 0.9em;
        }
        .verdict {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .verdict-local {
            background-color: #d4edda;
            color: #155724;
        }
        .verdict-server {
            background-color: #cce5ff;
            color: #004085;
        }
        .verdict-hybrid {
            background-color: #fff3cd;
            color: #856404;
        }
        a {
            color: #0066cc;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .source-list {
            font-size: 0.9em;
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin-top: 30px;
        }
        .toc {
            background: #fff;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        .toc li {
            margin-bottom: 5px;
        }
        .toc a {
            color: #4a4e69;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Data Resourcing Assessment</h1>
        <p><strong>Cuneiform Artifact Processing Pipeline</strong><br>
        Assessment Date: February 2026</p>

        <nav class="toc">
            <strong>Contents</strong>
            <ul>
                <li><a href="#executive-summary">Executive Summary</a></li>
                <li><a href="#data-sources">Primary Data Sources & APIs</a></li>
                <li><a href="#phase-analysis">Phase-by-Phase Analysis</a></li>
                <li><a href="#storage">Storage Requirements</a></li>
                <li><a href="#processing">Processing Architecture</a></li>
                <li><a href="#recommendations">Recommendations</a></li>
            </ul>
        </nav>

        <div class="section" id="executive-summary">
            <h2>Executive Summary</h2>

            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric">~390,000</div>
                    <div class="metric-label">Catalogued Artifacts (CDLI)</div>
                </div>
                <div class="metric-card">
                    <div class="metric">~50-200 GB</div>
                    <div class="metric-label">Full Image Archive Est.</div>
                </div>
                <div class="metric-card">
                    <div class="metric">60+</div>
                    <div class="metric-label">CDLI API Endpoints</div>
                </div>
                <div class="metric-card">
                    <div class="metric">~25,000</div>
                    <div class="metric-label">eBL Transliterated Tablets</div>
                </div>
            </div>

            <div class="summary-box">
                <strong>Key Finding:</strong> The pipeline can operate with a hybrid architecture where metadata and linguistic data are retrieved dynamically via APIs, while compute-intensive operations (OCR, ML inference) require either local GPU resources or cloud GPU services. Initial prototype can work with ~10-50 GB storage; production scale requires 200+ GB with proper caching.
            </div>

            <h3>Viability Assessment</h3>
            <table>
                <tr>
                    <th>Aspect</th>
                    <th>Status</th>
                    <th>Notes</th>
                </tr>
                <tr>
                    <td>Data Availability</td>
                    <td><span class="verdict verdict-local">Excellent</span></td>
                    <td>CDLI, ORACC, eBL all provide open access APIs and bulk downloads</td>
                </tr>
                <tr>
                    <td>API Access</td>
                    <td><span class="verdict verdict-local">Good</span></td>
                    <td>REST APIs available; no documented rate limits but courtesy limits apply</td>
                </tr>
                <tr>
                    <td>Image Storage</td>
                    <td><span class="verdict verdict-hybrid">Moderate</span></td>
                    <td>27-100+ MB per tablet (archival); 2-5 MB per tablet (web quality)</td>
                </tr>
                <tr>
                    <td>ML Model Access</td>
                    <td><span class="verdict verdict-server">Challenging</span></td>
                    <td>Open-source models exist but require GPU for inference</td>
                </tr>
            </table>
        </div>

        <div class="section" id="data-sources">
            <h2>Primary Data Sources & APIs</h2>

            <h3>1. CDLI (Cuneiform Digital Library Initiative)</h3>
            <p><a href="https://cdli.earth" target="_blank">https://cdli.earth</a></p>

            <table>
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Total Catalogued Artifacts</td>
                    <td>~390,000 inscribed artifacts</td>
                </tr>
                <tr>
                    <td>Photos/Drawings Available</td>
                    <td>Hundreds of thousands</td>
                </tr>
                <tr>
                    <td>Archaeological Sites</td>
                    <td>~700 proveniences</td>
                </tr>
                <tr>
                    <td>Collections Documented</td>
                    <td>~1,200 public/private</td>
                </tr>
                <tr>
                    <td>Bibliography Entries</td>
                    <td>~16,000 publications</td>
                </tr>
                <tr>
                    <td>External DB Links</td>
                    <td>400,000+ cross-references</td>
                </tr>
            </table>

            <h4>API Capabilities</h4>
            <ul>
                <li><strong>Endpoints:</strong> 60+ GET endpoints for metadata, linked data, bibliographies, inscriptions</li>
                <li><strong>Formats:</strong> JSON, JSON-LD, RDF/XML, N-Triples, Turtle, BibTeX, CSV, TSV, Excel, ATF</li>
                <li><strong>Authentication:</strong> Not required for public data</li>
                <li><strong>Rate Limits:</strong> Not documented; assume courtesy limits (~1 req/sec)</li>
                <li><strong>Bulk Download:</strong> Max 10,000 records per query; daily GitHub dumps available</li>
            </ul>

            <div class="info-box">
                <strong>GitHub Data Dump:</strong> <a href="https://github.com/cdli-gh/data">github.com/cdli-gh/data</a> provides daily catalogue and ATF transliteration dumps. Uses Git LFS for large files. Last major update: August 2022 (may be stale).
            </div>

            <h3>2. ORACC (Open Richly Annotated Cuneiform Corpus)</h3>
            <p><a href="https://oracc.museum.upenn.edu" target="_blank">https://oracc.museum.upenn.edu</a></p>

            <table>
                <tr>
                    <th>Feature</th>
                    <th>Details</th>
                </tr>
                <tr>
                    <td>Data Format</td>
                    <td>JSON with full lemmatization (sign-level detail)</td>
                </tr>
                <tr>
                    <td>Bulk Download</td>
                    <td><code>json.zip</code> per project via manifest</td>
                </tr>
                <tr>
                    <td>Access Method</td>
                    <td><code>/[PROJECT]/manifest.json</code> lists available files</td>
                </tr>
                <tr>
                    <td>License</td>
                    <td>Creative Commons Attribution Share-Alike 3.0</td>
                </tr>
                <tr>
                    <td>Languages Covered</td>
                    <td>Sumerian, Akkadian, other cuneiform languages</td>
                </tr>
            </table>

            <div class="success-box">
                <strong>Best for:</strong> Lemmatized text data, linguistic annotations, parallel corpus work. JSON format is well-documented and easy to parse.
            </div>

            <h3>3. eBL (Electronic Babylonian Literature)</h3>
            <p><a href="https://www.ebl.lmu.de" target="_blank">https://www.ebl.lmu.de</a></p>

            <table>
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Total Catalogue Records</td>
                    <td>262,717 tablets</td>
                </tr>
                <tr>
                    <td>Transliterated Tablets</td>
                    <td>~25,000 tablets</td>
                </tr>
                <tr>
                    <td>Lines of Text</td>
                    <td>350,000+ lines</td>
                </tr>
                <tr>
                    <td>Collections Covered</td>
                    <td>British Museum, Penn, Yale, Hilprecht</td>
                </tr>
            </table>

            <h4>API Features</h4>
            <ul>
                <li>Public API endpoint for full dataset download</li>
                <li>Python library for ATF parsing</li>
                <li>Data returned as single JSON file (list of fragment objects)</li>
                <li>GitHub repos: <code>ebl-api</code>, <code>cuneiform-ocr</code>, <code>ngram-matcher</code></li>
            </ul>

            <h3>4. ePSD2 (Electronic Pennsylvania Sumerian Dictionary)</h3>
            <p><a href="https://oracc.museum.upenn.edu/epsd2/sux" target="_blank">https://oracc.museum.upenn.edu/epsd2/sux</a></p>

            <table>
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Sumerian Words/Phrases</td>
                    <td>12,000+</td>
                </tr>
                <tr>
                    <td>Distinct Word Forms</td>
                    <td>~100,000</td>
                </tr>
                <tr>
                    <td>Total Attestations</td>
                    <td>2.27 million</td>
                </tr>
                <tr>
                    <td>Texts Covered</td>
                    <td>~100,000 of 134,000 known Sumerian texts</td>
                </tr>
            </table>

            <div class="warning-box">
                <strong>No dedicated API.</strong> Access via ORACC platform or web scraping. A Perl scraper exists: <a href="https://github.com/MMaiocchi/DictionaryScraper">MMaiocchi/DictionaryScraper</a>
            </div>

            <h3>5. CAD (Chicago Assyrian Dictionary)</h3>
            <p><a href="https://isac.uchicago.edu/research/publications/chicago-assyrian-dictionary" target="_blank">ISAC Chicago</a></p>

            <ul>
                <li><strong>Format:</strong> PDF only (26 volumes)</li>
                <li><strong>Access:</strong> Free download from ISAC; also on Internet Archive</li>
                <li><strong>API:</strong> None available</li>
                <li><strong>Usage:</strong> OCR processing required for programmatic access</li>
                <li><strong>Size:</strong> ~500 MB total (all volumes as PDF)</li>
            </ul>
        </div>

        <div class="section" id="phase-analysis">
            <h2>Phase-by-Phase Analysis</h2>

            <div class="phase-header">
                <span class="phase-number">1</span>
                <h3>Stage 1: Ingestion</h3>
            </div>
            <p><span class="verdict verdict-hybrid">Hybrid: Dynamic + Cached</span></p>

            <table>
                <tr>
                    <th>Resource</th>
                    <th>Volume</th>
                    <th>API Calls</th>
                    <th>Strategy</th>
                </tr>
                <tr>
                    <td>Artifact Metadata</td>
                    <td>1-5 KB per record</td>
                    <td>1 call per artifact</td>
                    <td>Dynamic fetch OK</td>
                </tr>
                <tr>
                    <td>Tablet Images (web)</td>
                    <td>2-5 MB each</td>
                    <td>1-6 calls per tablet (multi-view)</td>
                    <td>Cache locally</td>
                </tr>
                <tr>
                    <td>Tablet Images (archival)</td>
                    <td>27-100+ MB each (TIFF)</td>
                    <td>1-6 calls per tablet</td>
                    <td>Download on demand</td>
                </tr>
                <tr>
                    <td>PDF Catalogs</td>
                    <td>Variable (10-500 MB)</td>
                    <td>N/A (one-time download)</td>
                    <td>Cache locally</td>
                </tr>
            </table>

            <div class="info-box">
                <strong>Storage Estimate (1,000 artifacts):</strong><br>
                Web quality: 5-30 GB | Archival quality: 50-200 GB
            </div>

            <div class="phase-header">
                <span class="phase-number">2</span>
                <h3>Stage 2: Digitization & Recognition (OCR)</h3>
            </div>
            <p><span class="verdict verdict-server">Server/GPU Required</span></p>

            <table>
                <tr>
                    <th>Component</th>
                    <th>Requirement</th>
                    <th>Notes</th>
                </tr>
                <tr>
                    <td>DeepScribe Model</td>
                    <td>~500 MB - 2 GB model weights</td>
                    <td>RetinaNet + ResNet architecture</td>
                </tr>
                <tr>
                    <td>Training Dataset</td>
                    <td>5,000+ tablet images, 100K+ annotated signs</td>
                    <td>PFA dataset available</td>
                </tr>
                <tr>
                    <td>GPU Memory (inference)</td>
                    <td>4-8 GB VRAM minimum</td>
                    <td>RTX 3060 or better</td>
                </tr>
                <tr>
                    <td>Kraken OCR (alternative)</td>
                    <td>Smaller models, trainable</td>
                    <td>Needs 62K+ lines for training</td>
                </tr>
            </table>

            <div class="warning-box">
                <strong>Local vs Cloud:</strong> OCR inference requires GPU. Options:<br>
                1. Local GPU (RTX 3060+, ~$300-500 used)<br>
                2. Cloud GPU (AWS g5.xlarge: ~$1/hr with A10G)<br>
                3. Pre-process batches, cache results
            </div>

            <div class="phase-header">
                <span class="phase-number">3</span>
                <h3>Stage 3: Transliteration</h3>
            </div>
            <p><span class="verdict verdict-local">Local/Lightweight</span></p>

            <table>
                <tr>
                    <th>Resource</th>
                    <th>Size</th>
                    <th>Source</th>
                </tr>
                <tr>
                    <td>Sign-to-ATF Dictionary</td>
                    <td>~5-20 MB</td>
                    <td>CDLI, ORACC sign lists</td>
                </tr>
                <tr>
                    <td>Unicode Mapping Tables</td>
                    <td>&lt;1 MB</td>
                    <td>Unicode Consortium</td>
                </tr>
                <tr>
                    <td>Validation Rules</td>
                    <td>&lt;1 MB</td>
                    <td>ATF specification</td>
                </tr>
            </table>

            <p><strong>API Calls:</strong> Minimal; sign lists can be cached indefinitely.</p>
            <p><strong>Processing:</strong> Rule-based mapping, runs on CPU, &lt;1 sec per tablet.</p>

            <div class="phase-header">
                <span class="phase-number">4</span>
                <h3>Stage 4: Translation</h3>
            </div>
            <p><span class="verdict verdict-server">Server/GPU Required</span></p>

            <table>
                <tr>
                    <th>Model</th>
                    <th>Size</th>
                    <th>Performance</th>
                    <th>Requirements</th>
                </tr>
                <tr>
                    <td>Akkademia (T5-based)</td>
                    <td>~1-3 GB</td>
                    <td>36-37 BLEU score</td>
                    <td>GPU recommended</td>
                </tr>
                <tr>
                    <td>mT5/NLLB alternatives</td>
                    <td>2-10 GB</td>
                    <td>Variable</td>
                    <td>8-24 GB VRAM</td>
                </tr>
                <tr>
                    <td>Dictionary fallback</td>
                    <td>~50 MB (CAD subset)</td>
                    <td>Word-level only</td>
                    <td>CPU OK</td>
                </tr>
            </table>

            <div class="info-box">
                <strong>AICC Corpus:</strong> 130,000 pre-translated cuneiform texts available. For many use cases, fetching existing translations may be preferable to running inference.
            </div>

            <div class="phase-header">
                <span class="phase-number">5</span>
                <h3>Stage 5: Annotation & Enrichment</h3>
            </div>
            <p><span class="verdict verdict-hybrid">Hybrid: APIs + Local Processing</span></p>

            <table>
                <tr>
                    <th>Data Source</th>
                    <th>Access Method</th>
                    <th>API Calls per Artifact</th>
                </tr>
                <tr>
                    <td>ePSD2 (Sumerian lemmas)</td>
                    <td>ORACC JSON / scrape</td>
                    <td>5-50 (per unique word)</td>
                </tr>
                <tr>
                    <td>CAD (Akkadian glosses)</td>
                    <td>Local PDF/OCR cache</td>
                    <td>0 (pre-processed)</td>
                </tr>
                <tr>
                    <td>ORACC parallel texts</td>
                    <td>JSON API</td>
                    <td>1-10 (search + fetch)</td>
                </tr>
                <tr>
                    <td>eBL cross-references</td>
                    <td>REST API</td>
                    <td>1-5 (ngram matching)</td>
                </tr>
            </table>

            <p><strong>Local Storage:</strong> ~100 MB for lemma caches + graph structures</p>

            <div class="phase-header">
                <span class="phase-number">6</span>
                <h3>Stage 6: Contextualization</h3>
            </div>
            <p><span class="verdict verdict-hybrid">Hybrid: Web APIs + LLM</span></p>

            <table>
                <tr>
                    <th>Taxonomy Category</th>
                    <th>Primary Source</th>
                    <th>Retrieval Method</th>
                </tr>
                <tr>
                    <td>Historical Era/Period</td>
                    <td>Wikipedia, CDLI metadata</td>
                    <td>Web search + API</td>
                </tr>
                <tr>
                    <td>Geographic Provenance</td>
                    <td>CDLI, Pleiades</td>
                    <td>API + GeoJSON cache</td>
                </tr>
                <tr>
                    <td>People Involved</td>
                    <td>Prosopography databases</td>
                    <td>Specialized queries</td>
                </tr>
                <tr>
                    <td>Purpose/Function</td>
                    <td>Genre classifications</td>
                    <td>ORACC metadata</td>
                </tr>
                <tr>
                    <td>Cultural Context</td>
                    <td>Academic sources, X/Twitter</td>
                    <td>Web search, social APIs</td>
                </tr>
                <tr>
                    <td>Material/Preservation</td>
                    <td>Museum records</td>
                    <td>API or scrape</td>
                </tr>
                <tr>
                    <td>Modern Interpretations</td>
                    <td>Scholarly papers, social</td>
                    <td>Web search</td>
                </tr>
            </table>

            <div class="warning-box">
                <strong>API Call Volume:</strong> This stage is API-intensive. Expect 20-100 API calls per artifact across web search, Wikipedia, CDLI, and social media APIs. Consider aggressive caching and rate limiting.
            </div>

            <div class="phase-header">
                <span class="phase-number">7</span>
                <h3>Stage 7: Output & Archiving</h3>
            </div>
            <p><span class="verdict verdict-local">Local Processing</span></p>

            <table>
                <tr>
                    <th>Output Type</th>
                    <th>Size per Artifact</th>
                    <th>Processing</th>
                </tr>
                <tr>
                    <td>JSON artifact record</td>
                    <td>50-500 KB</td>
                    <td>CPU, &lt;1 sec</td>
                </tr>
                <tr>
                    <td>PDF report</td>
                    <td>1-5 MB</td>
                    <td>CPU, 2-5 sec</td>
                </tr>
                <tr>
                    <td>JSON-LD export</td>
                    <td>100-300 KB</td>
                    <td>CPU, &lt;1 sec</td>
                </tr>
                <tr>
                    <td>Timeline visualization</td>
                    <td>50-200 KB (SVG/PNG)</td>
                    <td>CPU, 1-3 sec</td>
                </tr>
            </table>
        </div>

        <div class="section" id="storage">
            <h2>Storage Requirements</h2>

            <h3>Minimum Viable Prototype (100 artifacts)</h3>
            <table>
                <tr>
                    <th>Component</th>
                    <th>Size</th>
                </tr>
                <tr>
                    <td>Tablet images (web quality)</td>
                    <td>500 MB - 1 GB</td>
                </tr>
                <tr>
                    <td>Metadata cache</td>
                    <td>10 MB</td>
                </tr>
                <tr>
                    <td>Sign dictionaries</td>
                    <td>50 MB</td>
                </tr>
                <tr>
                    <td>Translation model (optional local)</td>
                    <td>3 GB</td>
                </tr>
                <tr>
                    <td>OCR model (optional local)</td>
                    <td>2 GB</td>
                </tr>
                <tr>
                    <td>Output artifacts</td>
                    <td>100 MB</td>
                </tr>
                <tr>
                    <td><strong>Total</strong></td>
                    <td><strong>~5-7 GB</strong></td>
                </tr>
            </table>

            <h3>Production Scale (10,000 artifacts)</h3>
            <table>
                <tr>
                    <th>Component</th>
                    <th>Size</th>
                </tr>
                <tr>
                    <td>Tablet images (web quality)</td>
                    <td>50-100 GB</td>
                </tr>
                <tr>
                    <td>Tablet images (archival, subset)</td>
                    <td>100-500 GB</td>
                </tr>
                <tr>
                    <td>Metadata + transliterations</td>
                    <td>1 GB</td>
                </tr>
                <tr>
                    <td>Linguistic resources (all)</td>
                    <td>5 GB</td>
                </tr>
                <tr>
                    <td>ML models</td>
                    <td>10 GB</td>
                </tr>
                <tr>
                    <td>SSoT database (Neo4j + MongoDB)</td>
                    <td>20-50 GB</td>
                </tr>
                <tr>
                    <td>Output artifacts + cache</td>
                    <td>10 GB</td>
                </tr>
                <tr>
                    <td><strong>Total</strong></td>
                    <td><strong>~200-700 GB</strong></td>
                </tr>
            </table>

            <h3>Full CDLI Mirror (theoretical)</h3>
            <div class="warning-box">
                <strong>390,000 artifacts @ ~30 MB average (archival):</strong> ~12 TB storage<br>
                <strong>Not recommended.</strong> Use on-demand fetching with local cache.
            </div>
        </div>

        <div class="section" id="processing">
            <h2>Processing Architecture</h2>

            <h3>Local vs Server Decision Matrix</h3>
            <table>
                <tr>
                    <th>Stage</th>
                    <th>Local (CPU)</th>
                    <th>Local (GPU)</th>
                    <th>Cloud Service</th>
                    <th>Recommendation</th>
                </tr>
                <tr>
                    <td>1. Ingestion</td>
                    <td>Yes</td>
                    <td>-</td>
                    <td>-</td>
                    <td><span class="verdict verdict-local">Local</span></td>
                </tr>
                <tr>
                    <td>2. OCR</td>
                    <td>Slow</td>
                    <td>Fast</td>
                    <td>Fast</td>
                    <td><span class="verdict verdict-server">GPU/Cloud</span></td>
                </tr>
                <tr>
                    <td>3. Transliteration</td>
                    <td>Yes</td>
                    <td>-</td>
                    <td>-</td>
                    <td><span class="verdict verdict-local">Local</span></td>
                </tr>
                <tr>
                    <td>4. Translation</td>
                    <td>Very slow</td>
                    <td>Fast</td>
                    <td>Fast</td>
                    <td><span class="verdict verdict-hybrid">Hybrid</span></td>
                </tr>
                <tr>
                    <td>5. Annotation</td>
                    <td>Yes</td>
                    <td>-</td>
                    <td>APIs</td>
                    <td><span class="verdict verdict-hybrid">Hybrid</span></td>
                </tr>
                <tr>
                    <td>6. Context</td>
                    <td>Yes</td>
                    <td>-</td>
                    <td>Web APIs</td>
                    <td><span class="verdict verdict-hybrid">Hybrid</span></td>
                </tr>
                <tr>
                    <td>7. Output</td>
                    <td>Yes</td>
                    <td>-</td>
                    <td>-</td>
                    <td><span class="verdict verdict-local">Local</span></td>
                </tr>
            </table>

            <h3>GPU Requirements for ML Stages</h3>
            <table>
                <tr>
                    <th>Task</th>
                    <th>Min VRAM</th>
                    <th>Recommended</th>
                    <th>Speed (per tablet)</th>
                </tr>
                <tr>
                    <td>DeepScribe OCR</td>
                    <td>4 GB</td>
                    <td>8 GB</td>
                    <td>5-30 sec</td>
                </tr>
                <tr>
                    <td>Akkademia Translation</td>
                    <td>8 GB</td>
                    <td>16 GB</td>
                    <td>2-10 sec</td>
                </tr>
                <tr>
                    <td>Combined Pipeline</td>
                    <td>8 GB</td>
                    <td>24 GB</td>
                    <td>10-60 sec</td>
                </tr>
            </table>

            <div class="info-box">
                <strong>Cloud GPU Options:</strong><br>
                - AWS g5.xlarge (A10G, 24GB): ~$1.00/hr<br>
                - Lambda Labs (A10): ~$0.60/hr<br>
                - Google Colab Pro: ~$10/mo (limited hours)
            </div>

            <h3>Recommended Architecture</h3>
            <ol>
                <li><strong>Development/Prototype:</strong> Local machine with Docker, PostgreSQL, optional GPU for testing</li>
                <li><strong>Production:</strong>
                    <ul>
                        <li>Orchestration: Apache Airflow or Prefect (local or cloud)</li>
                        <li>Storage: S3-compatible object storage + PostgreSQL/MongoDB</li>
                        <li>ML Inference: Serverless GPU (Modal, Banana, RunPod) or batch processing</li>
                        <li>API calls: Rate-limited worker queue (Celery, RQ)</li>
                    </ul>
                </li>
            </ol>
        </div>

        <div class="section" id="recommendations">
            <h2>Recommendations</h2>

            <h3>Phase 1: Prototype (0-3 months)</h3>
            <ul>
                <li>Use CDLI GitHub dump for initial metadata/ATF</li>
                <li>Fetch images on-demand, cache locally (100-500 tablets)</li>
                <li>Skip ML stages; use existing ORACC/eBL transliterations</li>
                <li>Storage: 10 GB local SSD sufficient</li>
                <li>Cost: Free (APIs are open access)</li>
            </ul>

            <h3>Phase 2: MVP with ML (3-6 months)</h3>
            <ul>
                <li>Integrate DeepScribe or Kraken OCR</li>
                <li>Deploy translation model (Akkademia)</li>
                <li>Cloud GPU for batch processing (~$50-100/month)</li>
                <li>Storage: 50-100 GB</li>
                <li>Implement caching layer for API responses</li>
            </ul>

            <h3>Phase 3: Production Scale</h3>
            <ul>
                <li>Hybrid cloud architecture</li>
                <li>Pre-compute and cache common contextual data</li>
                <li>Implement SSoT with Neo4j + document store</li>
                <li>Storage: 500 GB - 1 TB</li>
                <li>Budget: ~$200-500/month cloud costs</li>
            </ul>

            <h3>Data Retrieval Strategy</h3>
            <table>
                <tr>
                    <th>Data Type</th>
                    <th>Strategy</th>
                    <th>Rationale</th>
                </tr>
                <tr>
                    <td>Artifact metadata</td>
                    <td>Dynamic + 24hr cache</td>
                    <td>Small, frequently updated</td>
                </tr>
                <tr>
                    <td>Transliterations</td>
                    <td>Cache indefinitely</td>
                    <td>Rarely changes</td>
                </tr>
                <tr>
                    <td>Images</td>
                    <td>Download on first access, keep</td>
                    <td>Large, stable</td>
                </tr>
                <tr>
                    <td>Sign dictionaries</td>
                    <td>Cache indefinitely</td>
                    <td>Static reference</td>
                </tr>
                <tr>
                    <td>Contextual data</td>
                    <td>Dynamic with 7-day cache</td>
                    <td>May be updated</td>
                </tr>
                <tr>
                    <td>ML model outputs</td>
                    <td>Cache per model version</td>
                    <td>Expensive to recompute</td>
                </tr>
            </table>
        </div>

        <div class="source-list">
            <h3>Sources</h3>
            <ul>
                <li><a href="https://cdli.earth/docs/api">CDLI REST API Documentation</a></li>
                <li><a href="https://github.com/cdli-gh/data">CDLI GitHub Data Repository</a></li>
                <li><a href="https://oracc.museum.upenn.edu/doc/opendata/json/">ORACC JSON Data Documentation</a></li>
                <li><a href="https://github.com/ElectronicBabylonianLiterature/ebl-api">eBL API Repository</a></li>
                <li><a href="https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.148">eBL Dataset Paper (JOHD)</a></li>
                <li><a href="https://github.com/oi-deepscribe/deepscribe">DeepScribe GitHub</a></li>
                <li><a href="https://arxiv.org/abs/2306.01268">DeepScribe Paper (arXiv)</a></li>
                <li><a href="https://academic.oup.com/pnasnexus/article/2/5/pgad096/7147349">Akkadian NMT Paper (PNAS Nexus)</a></li>
                <li><a href="https://kraken.re/">Kraken OCR Documentation</a></li>
                <li><a href="https://isac.uchicago.edu/research/publications/chicago-assyrian-dictionary">Chicago Assyrian Dictionary (ISAC)</a></li>
                <li><a href="https://www.cdli.earth/docs/images-acquisition-and-processing">CDLI Image Processing Guidelines</a></li>
                <li><a href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">NVIDIA LLM Inference Optimization</a></li>
            </ul>
        </div>

    </div>
</body>
</html>
